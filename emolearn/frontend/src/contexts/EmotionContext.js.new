import React, { createContext, useContext, useState, useEffect, useRef } from 'react';

const EmotionContext = createContext();

exebort const useEmotion = () => {
  const context = useContext(EmotionContext);
  if (!context) {
    throw new Error('useEmotion must be used within an EmotionProvider');
  }
  return context;
};

const EmotionProvider = ({ children, onEmotionChange }) => {
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const [modelsActuallyLoaded, setModelsActuallyLoaded] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState(null);
  const [emotionHistory, setEmotionHistory] = useState([]);
  const [isDetecting, setIsDetecting] = useState(false);
  const [cameraPermission, setCameraPermission] = useState('prompt');
  const [lastEmotionTime, setLastEmotionTime] = useState(null);
  const [isInCooldown, setIsInCooldown] = useState(false);
  const [recentDetections, setRecentDetections] = useState([]);
  const [error, setError] = useState(null);
  
  const videoRef = useRef(null);
  const streamRef = useRef(null);
  const intervalRef = useRef(null);
  const animationFrameRef = useRef(null);
  const cooldownRef = useRef(null);
  
  // Emotion detection configuration
  const CONFIDENCE_THRESHOLD = 0.6;
  const STABILITY_FRAMES = 3;
  const MIN_FACE_SIZE = 100;
  const DETECTION_INTERVAL = 300;
  const MODEL_URL = 'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.15/model';

  // Load face-api.js models on component mount
  useEffect(() => {
    const loadModels = async () => {
      setError('Loading AI models...');
      
      try {
        const faceapi = await import('@vladmandic/face-api');
        const MODEL_URL = 'https://justadudewhohacks.github.io/face-api.js/models';
        
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
          faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
          faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
        ]);
        
        setModelsActuallyLoaded(true);
        setError(null);
      } catch (error) {
        console.error('Error loading models:', error);
        setError('Failed to load face detection models. Please try refreshing the page.');
      } finally {
        setIsModelLoaded(true);
      }
    };

    loadModels();

    // Cleanup function
    return () => {
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (cooldownRef.current) {
        clearTimeout(cooldownRef.current);
      }
    };
  }, []);

  // Start cooldown period
  const startCooldown = () => {
    setIsInCooldown(true);
    cooldownRef.current = setTimeout(() => {
      setIsInCooldown(false);
    }, 5000); // 5 seconds cooldown
  };

  // Log emotion to the server
  const logEmotion = async (emotionData) => {
    try {
      const response = await fetch('/api/emotions/log', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(emotionData),
      });

      if (!response.ok) {
        throw new Error('Failed to log emotion');
      }

      return await response.json();
    } catch (err) {
      console.error('Error logging emotion:', err);
      throw err;
    }
  };

  // Get emotion-based content recommendation
  const getEmotionBasedAction = (emotion) => {
    const actions = {
      happiness: {
        action: 'positive_reinforcement',
        message: 'You\'re doing great! Your engagement is helping you learn more effectively.',
        suggestion: 'Try tackling a challenging problem while you\'re in this positive state!',
        icon: 'ðŸ˜Š',
        color: '#4CAF50',
        actions: [
          'Continue with current content',
          'Try a more challenging exercise',
          'Teach what you\'ve learned to someone else'
        ]
      },
      neutral: {
        action: 'encouragement',
        message: 'You\'re doing well! Let\'s keep the momentum going.',
        suggestion: 'Try to engage more with the material to enhance your learning experience.',
        icon: 'ðŸ˜',
        color: '#2196F3',
        actions: [
          'Ask a question',
          'Take notes',
          'Try a practice exercise'
        ]
      },
      sad: {
        action: 'support',
        message: 'It seems like you might be feeling a bit down. Learning can be challenging, but you\'ve got this!',
        suggestion: 'Take a short break and come back refreshed.',
        icon: 'ðŸ˜”',
        color: '#9C27B0',
        actions: [
          'Take a 5-minute break',
          'Try a different learning style',
          'Reach out for help'
        ]
      }
    };

    return actions[emotion] || {
      action: 'continue',
      message: 'Keep learning!',
      suggestion: 'Your progress is being tracked. Let me know if you need any help!',
      icon: 'ðŸ“š',
      color: '#607D8B',
      actions: [
        'Continue learning',
        'Review progress',
        'Ask for help'
      ]
    };
  };

  const contextValue = {
    isModelLoaded,
    modelsActuallyLoaded,
    currentEmotion,
    emotionHistory,
    isDetecting,
    cameraPermission,
    isInCooldown,
    error,
    videoRef,
    startEmotionDetection: () => {
      // Implementation for starting emotion detection
      console.log('Starting emotion detection');
    },
    stopEmotionDetection: () => {
      // Implementation for stopping emotion detection
      console.log('Stopping emotion detection');
    },
    getEmotionBasedAction
  };

  return (
    <EmotionContext.Provider value={contextValue}>
      {children}
    </EmotionContext.Provider>
  );
};

export default EmotionProvider;
